{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGqp8aIKbRAF36kDADK9iL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/rml_lab_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.99, strategy=\"epsilon_greedy\"):\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.strategy = strategy\n",
        "        self.epsilon = 0.1  # ε for ε-greedy\n",
        "        self.tau = 1.0      # temperature for Boltzmann\n",
        "        self.num_bins = 20\n",
        "        self.q_table = np.zeros((self.num_bins, self.num_bins, env.action_space.n))\n",
        "\n",
        "        # Create bins for discretization\n",
        "        self.pos_space = np.linspace(env.observation_space.low[0],\n",
        "                                     env.observation_space.high[0],\n",
        "                                     self.num_bins - 1)\n",
        "        self.vel_space = np.linspace(env.observation_space.low[1],\n",
        "                                     env.observation_space.high[1],\n",
        "                                     self.num_bins - 1)\n",
        "\n",
        "    def discretize(self, obs):\n",
        "        # The error \"TypeError: cannot unpack non-iterable numpy.float32 object\"\n",
        "        # suggests that 'obs' is sometimes a single scalar (e.g., numpy.float32)\n",
        "        # instead of the expected 2-element array [position, velocity].\n",
        "        # This can happen due to compatibility issues with older 'gym' versions\n",
        "        # and newer 'numpy' versions, as indicated by the warning messages.\n",
        "\n",
        "        # Ensure 'obs' is a 2-element array. If it's a scalar,\n",
        "        # assume it's the position and set velocity to 0.0 as a heuristic.\n",
        "        if isinstance(obs, (float, np.float32, int, np.int_)):\n",
        "            # If obs is a scalar, treat it as position and assume velocity is 0.0\n",
        "            # This is a heuristic to allow the code to run, but highlights a potential\n",
        "            # issue with the environment's observation return type.\n",
        "            obs_array = np.array([obs, 0.0])\n",
        "        elif isinstance(obs, np.ndarray) and obs.ndim == 0:\n",
        "            # Handle 0-dimensional numpy arrays (scalars wrapped in ndarray)\n",
        "            obs_array = np.array([obs.item(), 0.0])\n",
        "        elif isinstance(obs, (list, tuple, np.ndarray)) and len(obs) == 1:\n",
        "            # Handle 1-element iterables, assume it's position and velocity is 0.0\n",
        "            obs_array = np.array([obs[0], 0.0])\n",
        "        elif isinstance(obs, np.ndarray) and obs.shape == (2,):\n",
        "            # This is the expected case: a 2-element numpy array\n",
        "            obs_array = obs\n",
        "        else:\n",
        "            # For any other unexpected format, try a generic conversion\n",
        "            try:\n",
        "                temp_obs = np.asarray(obs).flatten()\n",
        "                if len(temp_obs) == 1:\n",
        "                    obs_array = np.array([temp_obs[0], 0.0])\n",
        "                elif len(temp_obs) >= 2:\n",
        "                    obs_array = temp_obs[:2] # Take the first two elements\n",
        "                else:\n",
        "                    raise ValueError(f\"Observation has fewer than 1 element: {temp_obs}\")\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not convert observation to a 2-element array. \"\n",
        "                                 f\"Original observation type: {type(obs)}, value: {obs}. Error: {e}\")\n",
        "\n",
        "        pos, vel = obs_array\n",
        "        pos_bin = np.digitize(pos, self.pos_space)\n",
        "        vel_bin = np.digitize(vel, self.vel_space)\n",
        "        # Ensure bins are within range\n",
        "        pos_bin = min(max(pos_bin, 0), self.num_bins - 1)\n",
        "        vel_bin = min(max(vel_bin, 0), self.num_bins - 1)\n",
        "        return (pos_bin, vel_bin)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if self.strategy == \"epsilon_greedy\":\n",
        "            if random.random() < self.epsilon:\n",
        "                return self.env.action_space.sample()\n",
        "            else:\n",
        "                return np.argmax(self.q_table[state])\n",
        "        elif self.strategy == \"boltzmann\":\n",
        "            q_values = self.q_table[state]\n",
        "            exp_q = np.exp(q_values / self.tau)\n",
        "            probs = exp_q / np.sum(exp_q)\n",
        "            return np.random.choice(len(q_values), p=probs)\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        best_next = np.max(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * best_next * (1 - done)\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.alpha * td_error\n",
        "\n",
        "def train_agent(strategy=\"epsilon_greedy\"):\n",
        "    # Disable the environment checker and enable new step API\n",
        "    env = gym.make(\"MountainCar-v0\", disable_env_checker=True, new_step_api=True)\n",
        "    agent = QLearningAgent(env, strategy=strategy)\n",
        "    rewards = []\n",
        "\n",
        "    for ep in range(300):\n",
        "        obs, _ = env.reset()\n",
        "        state = agent.discretize(obs)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            next_state = agent.discretize(next_obs)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            print(f\"{strategy} Episode {ep+1}: Avg Reward (last 50 eps) = {np.mean(rewards[-50:]):.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training with ε-greedy strategy:\")\n",
        "    train_agent(\"epsilon_greedy\")\n",
        "\n",
        "    print(\"\\nTraining with Boltzmann strategy:\")\n",
        "    train_agent(\"boltzmann\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ0qw6E5ft1_",
        "outputId": "5357ebe9-afa9-4287-9daa-96af89026ad3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with ε-greedy strategy:\n",
            "epsilon_greedy Episode 50: Avg Reward (last 50 eps) = -200.00\n",
            "epsilon_greedy Episode 100: Avg Reward (last 50 eps) = -200.00\n",
            "epsilon_greedy Episode 150: Avg Reward (last 50 eps) = -200.00\n",
            "epsilon_greedy Episode 200: Avg Reward (last 50 eps) = -200.00\n",
            "epsilon_greedy Episode 250: Avg Reward (last 50 eps) = -200.00\n",
            "epsilon_greedy Episode 300: Avg Reward (last 50 eps) = -200.00\n",
            "\n",
            "Training with Boltzmann strategy:\n",
            "boltzmann Episode 50: Avg Reward (last 50 eps) = -200.00\n",
            "boltzmann Episode 100: Avg Reward (last 50 eps) = -200.00\n",
            "boltzmann Episode 150: Avg Reward (last 50 eps) = -200.00\n",
            "boltzmann Episode 200: Avg Reward (last 50 eps) = -200.00\n",
            "boltzmann Episode 250: Avg Reward (last 50 eps) = -200.00\n",
            "boltzmann Episode 300: Avg Reward (last 50 eps) = -200.00\n"
          ]
        }
      ]
    }
  ]
}