{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKWGD9yOUKacA9kNQ6IWeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/lab_03_rml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2303A51271  batch-08"
      ],
      "metadata": {
        "id": "mcgadO8ibB2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMrflr5G3Ldf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "def epsilon_greedy(Q, s, nA, eps):\n",
        "    if np.random.rand() < eps:\n",
        "        return np.random.randint(nA)\n",
        "    qvals = Q[s]\n",
        "    return np.random.choice(np.flatnonzero(qvals == np.max(qvals)))\n",
        "\n",
        "def greedy_policy(Q):\n",
        "    return np.argmax(Q, axis=1)\n",
        "\n",
        "def evaluate(env, policy, episodes=1000):\n",
        "    total, success = 0, 0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done, ret = False, 0\n",
        "        while not done:\n",
        "            s, r, terminated, truncated, _ = env.step(policy[s])\n",
        "            done, ret = terminated or truncated, ret + r\n",
        "        total += ret\n",
        "        success += (ret > 0)\n",
        "    return total / episodes, success / episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def td0(env, policy_probs, alpha=0.1, gamma=0.99, episodes=20000):\n",
        "    nS = env.observation_space.n\n",
        "    V = np.zeros(nS)\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = np.random.choice(env.action_space.n, p=policy_probs[s])\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            V[s] += alpha * (r + (0 if done else gamma * V[s2]) - V[s])\n",
        "            s = s2\n",
        "    return V\n",
        "\n"
      ],
      "metadata": {
        "id": "LfnOibQk3TIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AXdX84u3bOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, alpha=0.1, gamma=0.99, eps1=1.0, eps2=0.05, episodes=30000):\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA))\n",
        "\n",
        "    def eps_at(ep): return eps2 + (eps1 - eps2) * (episodes - ep) / episodes\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        eps = eps_at(ep)\n",
        "        s, _ = env.reset()\n",
        "        a = epsilon_greedy(Q, s, nA, eps)\n",
        "        done = False\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if not done:\n",
        "                a2 = epsilon_greedy(Q, s2, nA, eps)\n",
        "                target = r + gamma * Q[s2, a2]\n",
        "            else:\n",
        "                target = r\n",
        "            Q[s, a] += alpha * (target - Q[s, a])\n",
        "            s, a = s2, (a2 if not done else 0)\n",
        "    return Q\n"
      ],
      "metadata": {
        "id": "_NQrmO3H29g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "\n",
        "    policy_probs = np.ones((nS, nA)) / nA\n",
        "    V = td0(env, policy_probs)\n",
        "    print(\"TD(0) Values:\\n\", V.reshape(4, 4))\n",
        "\n",
        "    Q = sarsa(env)\n",
        "    policy = greedy_policy(Q)\n",
        "    print(\"\\nSARSA Greedy Policy:\\n\", policy.reshape(4, 4))\n",
        "\n",
        "    avg, succ = evaluate(env, policy, 5000)\n",
        "    print(f\"\\nAvg return={avg:.3f}, Success={succ*100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMI5JmH63eVu",
        "outputId": "4de92445-7f11-4a51-caff-db3a3f553657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD(0) Values:\n",
            " [[0.01128485 0.0052601  0.01131197 0.00414983]\n",
            " [0.01689198 0.         0.02325782 0.        ]\n",
            " [0.02464562 0.06178836 0.08466198 0.        ]\n",
            " [0.         0.12051186 0.3863904  0.        ]]\n",
            "\n",
            "SARSA Greedy Policy:\n",
            " [[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            "\n",
            "Avg return=0.721, Success=72.08%\n"
          ]
        }
      ]
    }
  ]
}