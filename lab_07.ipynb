{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPziA9XV5sEfXvxnJ3sGQlA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/lab_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2303A51271\n",
        "p.manikanta\n",
        "\n",
        "Batch-08"
      ],
      "metadata": {
        "id": "WxzaG_aHGqFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfBWZ_iOEfR3",
        "outputId": "f245d910-3d3d-4852-9c45-b70c14033c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  200  Average reward (last 200): 0.460\n",
            "Episode  400  Average reward (last 200): 0.955\n",
            "Episode  600  Average reward (last 200): 0.990\n",
            "Episode  800  Average reward (last 200): 0.990\n",
            "Episode 1000  Average reward (last 200): 1.000\n",
            "Episode 1200  Average reward (last 200): 1.000\n",
            "Episode 1400  Average reward (last 200): 0.995\n",
            "Episode 1600  Average reward (last 200): 1.000\n",
            "Episode 1800  Average reward (last 200): 1.000\n",
            "Episode 2000  Average reward (last 200): 1.000\n",
            "Done. Last 20 episode rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ToyEnv:\n",
        "    def __init__(self, target=10, max_steps=20, seed=0):\n",
        "        self.target = target\n",
        "        self.max_steps = max_steps\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.pos = 0\n",
        "        self.steps = 0\n",
        "        return np.array([self.pos], dtype=np.float32)\n",
        "    def step(self, action):\n",
        "        self.pos += (1 if action==1 else -1)\n",
        "        self.steps += 1\n",
        "        done = False\n",
        "        reward = 0.0\n",
        "        if self.pos >= self.target:\n",
        "            done = True\n",
        "            reward = 1.0\n",
        "        elif self.steps >= self.max_steps:\n",
        "            done = True\n",
        "        return np.array([self.pos], dtype=np.float32), reward, done, {}\n",
        "\n",
        "class SoftmaxPolicy:\n",
        "    def __init__(self, state_dim, n_actions, lr=1e-2, seed=1):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = lr\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.W = 0.01 * self.rng.randn(n_actions, state_dim+1)\n",
        "    def _featurize(self, s):\n",
        "        return np.concatenate([s, [1.0]])\n",
        "    def action_probs(self, s):\n",
        "        x = self._featurize(s)\n",
        "        logits = self.W.dot(x)\n",
        "        logits = logits - np.max(logits)\n",
        "        exp = np.exp(logits)\n",
        "        return exp / np.sum(exp)\n",
        "    def sample(self, s):\n",
        "        p = self.action_probs(s)\n",
        "        return self.rng.choice(self.n_actions, p=p), p\n",
        "    def update_episode(self, states, actions, returns):\n",
        "        for s, a, G in zip(states, actions, returns):\n",
        "            x = self._featurize(s)\n",
        "            probs = self.action_probs(s)\n",
        "            grad_log = -probs[:,None] * x[None,:]\n",
        "            grad_log[a] += x\n",
        "            self.W += self.lr * G * grad_log\n",
        "\n",
        "def discount_rewards(rewards, gamma):\n",
        "    Gs = np.zeros_like(rewards, dtype=np.float32)\n",
        "    G = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G\n",
        "        Gs[t] = G\n",
        "    return Gs\n",
        "\n",
        "def run_vanilla_reinforce(seed=0):\n",
        "    env = ToyEnv(seed=seed)\n",
        "    policy = SoftmaxPolicy(state_dim=1, n_actions=2, lr=0.01, seed=seed+1)\n",
        "    n_episodes = 2000\n",
        "    gamma = 0.99\n",
        "    rewards_history = []\n",
        "    for ep in range(1, n_episodes+1):\n",
        "        s = env.reset()\n",
        "        states, actions, rewards = [], [], []\n",
        "        done = False\n",
        "        while not done:\n",
        "            a, _ = policy.sample(s)\n",
        "            a_mapped = 1 if a==1 else -1\n",
        "            ns, r, done, _ = env.step(a_mapped)\n",
        "            states.append(s.copy())\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            s = ns\n",
        "        returns = discount_rewards(rewards, gamma)\n",
        "        policy.update_episode(states, actions, returns)\n",
        "        ep_reward = sum(rewards)\n",
        "        rewards_history.append(ep_reward)\n",
        "        if ep % 200 == 0:\n",
        "            avg = np.mean(rewards_history[-200:])\n",
        "            print(f\"Episode {ep:4d}  Average reward (last 200): {avg:.3f}\")\n",
        "    return rewards_history, policy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rewards, policy = run_vanilla_reinforce(seed=42)\n",
        "    print(\"Done. Last 20 episode rewards:\", rewards[-20:])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ToyEnv:\n",
        "    def __init__(self, target=10, max_steps=20, seed=0):\n",
        "        self.target = target\n",
        "        self.max_steps = max_steps\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.pos = 0\n",
        "        self.steps = 0\n",
        "        return np.array([self.pos], dtype=np.float32)\n",
        "    def step(self, action):\n",
        "        self.pos += (1 if action==1 else -1)\n",
        "        self.steps += 1\n",
        "        done = False\n",
        "        reward = 0.0\n",
        "        if self.pos >= self.target:\n",
        "            done = True\n",
        "            reward = 1.0\n",
        "        elif self.steps >= self.max_steps:\n",
        "            done = True\n",
        "        return np.array([self.pos], dtype=np.float32), reward, done, {}\n",
        "\n",
        "class SoftmaxPolicy:\n",
        "    def __init__(self, state_dim, n_actions, lr=5e-3, seed=1):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = lr\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.W = 0.01 * self.rng.randn(n_actions, state_dim+1)\n",
        "    def _featurize(self, s):\n",
        "        return np.concatenate([s, [1.0]])\n",
        "    def action_probs(self, s):\n",
        "        x = self._featurize(s)\n",
        "        logits = self.W.dot(x)\n",
        "        logits = logits - np.max(logits)\n",
        "        exp = np.exp(logits)\n",
        "        return exp / np.sum(exp)\n",
        "    def sample(self, s):\n",
        "        p = self.action_probs(s)\n",
        "        return self.rng.choice(self.n_actions, p=p), p\n",
        "    def update(self, grad_log_sums):\n",
        "        self.W += self.lr * grad_log_sums\n",
        "\n",
        "class LinearBaseline:\n",
        "    def __init__(self, state_dim, lr=1e-2, seed=2):\n",
        "        self.state_dim = state_dim\n",
        "        self.lr = lr\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.w = 0.01 * self.rng.randn(state_dim+1)\n",
        "    def featurize(self, s):\n",
        "        return np.concatenate([s, [1.0]])\n",
        "    def predict(self, s):\n",
        "        return float(self.w.dot(self.featurize(s)))\n",
        "    def update(self, states, returns):\n",
        "        for s, G in zip(states, returns):\n",
        "            x = self.featurize(s)\n",
        "            pred = self.w.dot(x)\n",
        "            grad = (G - pred) * x\n",
        "            self.w += self.lr * grad\n",
        "\n",
        "def discount_rewards(rewards, gamma):\n",
        "    Gs = np.zeros_like(rewards, dtype=np.float32)\n",
        "    G = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G\n",
        "        Gs[t] = G\n",
        "    return Gs\n",
        "\n",
        "def run_reinforce_with_baseline(seed=0):\n",
        "    env = ToyEnv(seed=seed)\n",
        "    policy = SoftmaxPolicy(state_dim=1, n_actions=2, lr=0.005, seed=seed+1)\n",
        "    baseline = LinearBaseline(state_dim=1, lr=0.05, seed=seed+2)\n",
        "    n_episodes = 2000\n",
        "    gamma = 0.99\n",
        "    rewards_history = []\n",
        "    for ep in range(1, n_episodes+1):\n",
        "        s = env.reset()\n",
        "        states, actions, rewards = [], [], []\n",
        "        done = False\n",
        "        while not done:\n",
        "            a, _ = policy.sample(s)\n",
        "            a_mapped = 1 if a==1 else -1\n",
        "            ns, r, done, _ = env.step(a_mapped)\n",
        "            states.append(s.copy())\n",
        "            actions.append(a)\n",
        "            rewards.append(r)\n",
        "            s = ns\n",
        "        returns = discount_rewards(rewards, gamma)\n",
        "        adv = np.array([returns[i] - baseline.predict(states[i]) for i in range(len(states))], dtype=np.float32)\n",
        "        grad_sum = np.zeros_like(policy.W)\n",
        "        for s0, a0, A in zip(states, actions, adv):\n",
        "            x = policy._featurize(s0)\n",
        "            probs = policy.action_probs(s0)\n",
        "            grad_log = -probs[:,None] * x[None,:]\n",
        "            grad_log[a0] += x\n",
        "            grad_sum += A * grad_log\n",
        "        policy.update(grad_sum)\n",
        "        baseline.update(states, returns)\n",
        "        ep_reward = sum(rewards)\n",
        "        rewards_history.append(ep_reward)\n",
        "        if ep % 200 == 0:\n",
        "            avg = np.mean(rewards_history[-200:])\n",
        "            print(f\"Episode {ep:4d}  Average reward (last 200): {avg:.3f}\")\n",
        "    return rewards_history, policy, baseline\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rewards_b, policy_b, baseline = run_reinforce_with_baseline(seed=123)\n",
        "    print(\"Done. Last 20 episode rewards:\", rewards_b[-20:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjR49Xh9F2S9",
        "outputId": "0381da64-b7f2-4ab7-98df-f8318c3465df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  200  Average reward (last 200): 0.010\n",
            "Episode  400  Average reward (last 200): 0.000\n",
            "Episode  600  Average reward (last 200): 0.000\n",
            "Episode  800  Average reward (last 200): 0.000\n",
            "Episode 1000  Average reward (last 200): 0.000\n",
            "Episode 1200  Average reward (last 200): 0.000\n",
            "Episode 1400  Average reward (last 200): 0.000\n",
            "Episode 1600  Average reward (last 200): 0.000\n",
            "Episode 1800  Average reward (last 200): 0.000\n",
            "Episode 2000  Average reward (last 200): 0.000\n",
            "Done. Last 20 episode rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    }
  ]
}