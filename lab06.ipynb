{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY1Xnb7+c9MgMNyDpa87Q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing Deep Q-Networks -DQN for Atari games or similar environments"
      ],
      "metadata": {
        "id": "6qbBVKPFEo-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pAsUTNfS96S2"
      },
      "outputs": [],
      "source": [
        "# PART 1: Imports, Replay Buffer, and Q-Network (Gymnasium compatible)\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import gymnasium as gym   # ✅ Use Gymnasium instead of Gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Transition tuple for replay buffer\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states = torch.tensor([b.state for b in batch], dtype=torch.float32)\n",
        "        actions = torch.tensor([b.action for b in batch], dtype=torch.int64)\n",
        "        rewards = torch.tensor([b.reward for b in batch], dtype=torch.float32)\n",
        "        next_states = torch.tensor([b.next_state for b in batch], dtype=torch.float32)\n",
        "        dones = torch.tensor([b.done for b in batch], dtype=torch.float32)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Q-Network (MLP for CartPole)\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: DQN Agent\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, device):\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        # Networks\n",
        "        self.online = QNetwork(state_dim, n_actions).to(device)\n",
        "        self.target = QNetwork(state_dim, n_actions).to(device)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Optimizer & Replay Buffer\n",
        "        self.optimizer = optim.Adam(self.online.parameters(), lr=1e-3)\n",
        "        self.replay = ReplayBuffer(50000)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.target_update_freq = 1000\n",
        "        self.min_replay_size = 1000\n",
        "        self.epsilon_start = 1.0\n",
        "        self.epsilon_final = 0.01\n",
        "        self.epsilon_decay = 5000\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        eps = self.epsilon_final + (self.epsilon_start - self.epsilon_final) * \\\n",
        "              np.exp(-1.0 * self.steps_done / self.epsilon_decay)\n",
        "        self.steps_done += 1\n",
        "        if random.random() < eps:\n",
        "            return random.randrange(self.n_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "                qvals = self.online(state_t)\n",
        "                return int(torch.argmax(qvals, dim=1).item())\n",
        "\n",
        "    def compute_td_loss(self):\n",
        "        states, actions, rewards, next_states, dones = self.replay.sample(self.batch_size)\n",
        "\n",
        "        # Move to device\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "\n",
        "        # Q(s,a)\n",
        "        q_values = self.online(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Target Q\n",
        "        with torch.no_grad():\n",
        "            max_next_q = self.target(next_states).max(1)[0]\n",
        "            td_target = rewards + self.gamma * (1 - dones) * max_next_q\n",
        "\n",
        "        loss = F.mse_loss(q_values, td_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.online.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "        return loss.item()"
      ],
      "metadata": {
        "id": "nCPPyz95GR6A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(env_id=\"CartPole-v1\", num_frames=20000, log_interval=1000):\n",
        "    env = gym.make(env_id)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    agent = DQNAgent(\n",
        "        state_dim=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Reset handling for new/old Gym API\n",
        "    reset_output = env.reset()\n",
        "    if isinstance(reset_output, tuple):   # new API\n",
        "        state, _ = reset_output\n",
        "    else:                                 # old API\n",
        "        state = reset_output\n",
        "\n",
        "    episode_reward = 0\n",
        "    all_rewards, losses = [], []\n",
        "\n",
        "    for frame_idx in range(1, num_frames + 1):\n",
        "        action = agent.select_action(state)\n",
        "        step_output = env.step(action)\n",
        "\n",
        "        # Handle both API versions\n",
        "        if len(step_output) == 5:  # new API\n",
        "            next_state, reward, terminated, truncated, _ = step_output\n",
        "            done = terminated or truncated\n",
        "        else:                      # old API\n",
        "            next_state, reward, done, _ = step_output\n",
        "\n",
        "        agent.replay.push(state, action, reward, next_state, float(done))\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        if len(agent.replay) > agent.min_replay_size:\n",
        "            loss = agent.compute_td_loss()\n",
        "            losses.append(loss)\n",
        "\n",
        "        # Update target\n",
        "        if frame_idx % agent.target_update_freq == 0:\n",
        "            agent.target.load_state_dict(agent.online.state_dict())\n",
        "\n",
        "        # End of episode\n",
        "        if done:\n",
        "            reset_output = env.reset()\n",
        "            state = reset_output[0] if isinstance(reset_output, tuple) else reset_output\n",
        "            all_rewards.append(episode_reward)\n",
        "            episode_reward = 0\n",
        "\n",
        "        # Logging\n",
        "        if frame_idx % log_interval == 0:\n",
        "            avg_reward = np.mean(all_rewards[-10:]) if all_rewards else 0.0\n",
        "            avg_loss = np.mean(losses[-100:]) if losses else 0.0\n",
        "            print(f\"Frame {frame_idx}, AvgReward(10) {avg_reward:.2f}, AvgLoss(100) {avg_loss:.4f}\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, all_rewards, losses"
      ],
      "metadata": {
        "id": "A6HIL3TvGV31"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    trained_agent, rewards, losses = train_dqn(\n",
        "        env_id=\"CartPole-v1\",\n",
        "        num_frames=20000,\n",
        "        log_interval=2000\n",
        "    )\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    print(\"Last 10 episode rewards:\", rewards[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZB2IQAQGZgd",
        "outputId": "b0636fc6-ba89-4dea-fdf8-f17f8e29e0e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1425822891.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor([b.state for b in batch], dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 2000, AvgReward(10) 17.10, AvgLoss(100) 0.0003\n",
            "Frame 4000, AvgReward(10) 18.30, AvgLoss(100) 0.0356\n",
            "Frame 6000, AvgReward(10) 62.90, AvgLoss(100) 0.0322\n",
            "Frame 8000, AvgReward(10) 109.80, AvgLoss(100) 0.0716\n",
            "Frame 10000, AvgReward(10) 152.80, AvgLoss(100) 0.0958\n",
            "Frame 12000, AvgReward(10) 134.90, AvgLoss(100) 0.2214\n",
            "Frame 14000, AvgReward(10) 146.70, AvgLoss(100) 0.1648\n",
            "Frame 16000, AvgReward(10) 149.80, AvgLoss(100) 0.2039\n",
            "Frame 18000, AvgReward(10) 211.90, AvgLoss(100) 0.2699\n",
            "Frame 20000, AvgReward(10) 138.00, AvgLoss(100) 0.2193\n",
            "Training complete!\n",
            "Last 10 episode rewards: [129.0, 284.0, 129.0, 119.0, 114.0, 126.0, 123.0, 131.0, 115.0, 110.0]\n"
          ]
        }
      ]
    }
  ]
}