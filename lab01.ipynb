{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiLOCNUvhThnPi38vCL4l9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox1GToB-FA6G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GridWorld parameters\n",
        "N = 5  # Grid size\n",
        "gamma = 0.9  # Discount factor\n",
        "goal = (4, 4)\n",
        "pit = (2, 2)\n",
        "\n",
        "# Initialize value function\n",
        "V = np.zeros((N, N))\n",
        "\n",
        "# Define reward function\n",
        "def reward(state):\n",
        "    if state == goal:\n",
        "        return 10\n",
        "    elif state == pit:\n",
        "        return -10\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Define deterministic policy: Right if possible, else Down\n",
        "def next_state(state):\n",
        "    x, y = state\n",
        "    if y < N - 1:\n",
        "        return (x, y + 1)\n",
        "    elif x < N - 1:\n",
        "        return (x + 1, y)\n",
        "    else:\n",
        "        return (x, y)  # Terminal state\n",
        "\n",
        "# Value iteration under fixed policy\n",
        "def compute_value_function(V, gamma, threshold=1e-4):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for x in range(N):\n",
        "            for y in range(N):\n",
        "                s = (x, y)\n",
        "                s_prime = next_state(s)\n",
        "                r = reward(s_prime)\n",
        "                v_new = r + gamma * V[s_prime]\n",
        "                delta = max(delta, abs(v_new - V[s]))\n",
        "                V[s] = v_new\n",
        "        if delta < threshold:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# Run value function computation\n",
        "V = compute_value_function(V, gamma)\n",
        "\n",
        "# Display value function\n",
        "print(\"Computed Value Function V(s):\")\n",
        "for row in V:\n",
        "    print([\"{0:.2f}\".format(val) for val in row])\n",
        "\n",
        "# Visualize value function\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(V, cmap='coolwarm', interpolation='none')\n",
        "plt.colorbar(label='Value')\n",
        "plt.title('Value Function Heatmap')\n",
        "plt.xticks(np.arange(N))\n",
        "plt.yticks(np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "Computed Value Function V(s):\n",
        "['42.61', '48.46', '54.95', '62.17', '70.19']\n",
        "['48.46', '54.95', '62.17', '70.19', '79.10']\n",
        "['46.85', '53.17', '70.19', '79.10', '89.00']\n",
        "['62.17', '70.19', '79.10', '89.00', '100.00']\n",
        "['70.19', '79.10', '89.00', '100.00', '100.00']\n",
        "\n",
        "\n",
        "# Simple MDP definition\n",
        "states = ['s0', 's1', 's2']\n",
        "actions = ['a0', 'a1']\n",
        "terminal_state = 's2'\n",
        "\n",
        "# Transition model: P[state][action] = list of (prob, next_state, reward)\n",
        "P = {\n",
        "    's0': {\n",
        "        'a0': [(1.0, 's1', 5)],\n",
        "        'a1': [(1.0, 's2', 10)]\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': [(1.0, 's0', -1)],\n",
        "        'a1': [(1.0, 's2', 2)]\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': [(1.0, 's2', 0)],\n",
        "        'a1': [(1.0, 's2', 0)]\n",
        "    }\n",
        "}\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.001\n",
        "\n",
        "# ------------------ Value Iteration ------------------\n",
        "def value_iteration(P, states, actions, gamma=0.9, theta=0.001):\n",
        "    V = {s: 0 for s in states}\n",
        "    policy = {s: actions[0] for s in states}\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            action_values = []\n",
        "            for a in actions:\n",
        "                value = sum([prob * (reward + gamma * V[next_state])\n",
        "                             for prob, next_state, reward in P[s][a]])\n",
        "                action_values.append((value, a))\n",
        "            max_value, best_action = max(action_values)\n",
        "            delta = max(delta, abs(V[s] - max_value))\n",
        "            V[s] = max_value\n",
        "            policy[s] = best_action\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "# ------------------ Policy Iteration ------------------\n",
        "def policy_evaluation(policy, P, states, gamma=0.9, theta=0.001):\n",
        "    V = {s: 0 for s in states}\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            a = policy[s]\n",
        "            v = sum([prob * (reward + gamma * V[next_state])\n",
        "                     for prob, next_state, reward in P[s][a]])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration(P, states, actions, gamma=0.9, theta=0.001):\n",
        "    policy = {s: actions[0] for s in states}\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, P, states, gamma, theta)\n",
        "        policy_stable = True\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            old_action = policy[s]\n",
        "            action_values = {\n",
        "                a: sum([prob * (reward + gamma * V[next_state])\n",
        "                        for prob, next_state, reward in P[s][a]])\n",
        "                for a in actions\n",
        "            }\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            policy[s] = best_action\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "# ------------------ Run Both Algorithms ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"游대 Value Iteration:\")\n",
        "    vi_policy, vi_values = value_iteration(P, states, actions)\n",
        "    print(\"Policy:\", vi_policy)\n",
        "    print(\"Values:\", vi_values)\n",
        "\n",
        "    print(\"\\n游대 Policy Iteration:\")\n",
        "    pi_policy, pi_values = policy_iteration(P, states, actions)\n",
        "    print(\"Policy:\", pi_policy)\n",
        "    print(\"Values:\", pi_values)\n",
        "\n",
        "\n",
        "游대 Value Iteration:\n",
        "Policy: {'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n",
        "Values: {'s0': 21.575091698945428, 's1': 18.417582529050886, 's2': 0}\n",
        "\n",
        "游대 Policy Iteration:\n",
        "Policy: {'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n",
        "Values: {'s0': 21.575325291175456, 's1': 18.41779276205791, 's2': 0}\n",
        "\n",
        "# Define the states of the MDP. In this case, representing different levels of solar power availability.\n",
        "# Increased to 5 states\n",
        "states = [\"Very Low Solar\", \"Low Solar\", \"Medium Solar\", \"High Solar\", \"Very High Solar\"]\n",
        "\n",
        "# Define the actions the agent can take in each state.\n",
        "actions = [\"Activate Generator\", \"Do Not Activate Generator\"]\n",
        "\n",
        "# Define transition probabilities (conceptual example).\n",
        "# This is a dictionary where:\n",
        "# - The first level key is the current state (s).\n",
        "# - The second level key is the action taken (a).\n",
        "# - The third level key is the next state (s').\n",
        "# - The value is the probability of transitioning from s to s' given action a (P(s'|s,a)).\n",
        "# These probabilities would ideally be derived from historical data or a system model.\n",
        "# *** NOTE: You will need to fill in the specific probabilities for the new states\n",
        "# and adjust existing ones as needed for your 5-state model. ***\n",
        "transition_probabilities = {\n",
        "    \"Very Low Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.6, \"Low Solar\": 0.3, \"Medium Solar\": 0.1, \"High Solar\": 0.0, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.7, \"Low Solar\": 0.2, \"Medium Solar\": 0.1, \"High Solar\": 0.0, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"Low Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.1, \"Low Solar\": 0.5, \"Medium Solar\": 0.3, \"High Solar\": 0.1, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.1, \"Low Solar\": 0.6, \"Medium Solar\": 0.2, \"High Solar\": 0.1, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"Medium Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.2, \"Medium Solar\": 0.6, \"High Solar\": 0.2, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.3, \"Medium Solar\": 0.5, \"High Solar\": 0.2, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"High Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.1, \"Medium Solar\": 0.3, \"High Solar\": 0.5, \"Very High Solar\": 0.1\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.05, \"Medium Solar\": 0.15, \"High Solar\": 0.6, \"Very High Solar\": 0.2\n",
        "        }\n",
        "    },\n",
        "    \"Very High Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.0, \"Medium Solar\": 0.1, \"High Solar\": 0.3, \"Very High Solar\": 0.6\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.0, \"Medium Solar\": 0.05, \"High Solar\": 0.15, \"Very High Solar\": 0.8\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the reward function (conceptual example).\n",
        "# This is a dictionary where:\n",
        "# - The first level key is the current state (s).\n",
        "# - The second level key is the action taken (a).\n",
        "# - The third level key is the next state (s').\n",
        "# - The value is the immediate reward received when taking action a in state s and transitioning to state s' (R(s,a,s')).\n",
        "# The rewards are defined to incentivize meeting demand efficiently.\n",
        "# Assuming High Solar and Very High Solar meet demand alone (+10 reward when generator is off).\n",
        "# Medium Solar might sometimes meet demand (+5 reward when generator is off), Low/Very Low do not (-5 penalty when generator is off).\n",
        "# Activating the generator always meets demand (+5 reward), regardless of solar state.\n",
        "# *** NOTE: You will need to fill in the specific reward values for the new states\n",
        "# and adjust existing ones as needed for your 5-state model. ***\n",
        "reward_function = {\n",
        "    \"Very Low Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: -5 for s_prime in states} # No generator, likely unmet demand\n",
        "    },\n",
        "    \"Low Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: -5 for s_prime in states} # No generator, likely unmet demand\n",
        "    },\n",
        "    \"Medium Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {\n",
        "             \"Very Low Solar\": -5, \"Low Solar\": -5, \"Medium Solar\": 5, \"High Solar\": 10, \"Very High Solar\": 10\n",
        "        } # Medium might meet demand sometimes, High/Very High do\n",
        "    },\n",
        "    \"High Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: 10 for s_prime in states} # High solar meets demand\n",
        "    },\n",
        "    \"Very High Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: 10 for s_prime in states} # Very High solar meets demand\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the discount factor (gamma).\n",
        "# This value (between 0 and 1) determines the present value of future rewards.\n",
        "# A higher discount factor means future rewards are considered more important.\n",
        "discount_factor = 0.9"
      ]
    }
  ]
}