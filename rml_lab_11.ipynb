{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXu3hM6Wxlwb/P13WxA5iK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/rml_lab_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8igjw0EcZwZv",
        "outputId": "31d38aa2-6352-4be4-acfb-bd09b8495b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Epoch 0, Loss: 0.7202\n",
            "Epoch 10, Loss: 0.6901\n",
            "Epoch 20, Loss: 0.6724\n",
            "Epoch 30, Loss: 0.6532\n",
            "Epoch 40, Loss: 0.6332\n",
            "Epoch 50, Loss: 0.6117\n",
            "Epoch 60, Loss: 0.5883\n",
            "Epoch 70, Loss: 0.5630\n",
            "Epoch 80, Loss: 0.5357\n",
            "Epoch 90, Loss: 0.5066\n",
            "Average Reward (Behavioral Cloning): 103.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import gymnasium as gym # Changed to gymnasium\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Neural Network Policy\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, act_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def act(self, obs):\n",
        "        logits = self.forward(obs)\n",
        "        return torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "# Generate Expert Demonstrations (using a simple heuristic)\n",
        "def generate_expert_data(env, num_episodes=10):\n",
        "    data_obs, data_act = [], []\n",
        "    for ep in range(num_episodes):\n",
        "        obs, _ = env.reset() # gymnasium.reset() returns (observation, info)\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Expert heuristic: push toward the falling direction\n",
        "            angle, angle_vel = obs[2], obs[3]\n",
        "            action = 0 if angle < 0 else 1\n",
        "            data_obs.append(obs)\n",
        "            data_act.append(action)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action) # gymnasium.step() returns (obs, reward, terminated, truncated, info)\n",
        "            done = terminated or truncated # 'done' is now either terminated or truncated\n",
        "    return np.array(data_obs), np.array(data_act)\n",
        "\n",
        "# Train Behavioral Cloning Policy\n",
        "def train_behavioral_cloning():\n",
        "    env = gym.make(\"CartPole-v1\") # gymnasium.make() defaults to new API\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    expert_obs, expert_act = generate_expert_data(env)\n",
        "    policy = PolicyNet(obs_dim, act_dim)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X = torch.tensor(expert_obs, dtype=torch.float32)\n",
        "    y = torch.tensor(expert_act, dtype=torch.long)\n",
        "\n",
        "    # Train imitation model\n",
        "    for epoch in range(100):\n",
        "        logits = policy(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    total_rewards = []\n",
        "    for ep in range(5):\n",
        "        obs, _ = env.reset() # gymnasium.reset() returns (observation, info)\n",
        "        done, ep_reward = False, 0\n",
        "        while not done:\n",
        "            action = policy.act(torch.tensor(obs, dtype=torch.float32))\n",
        "            obs, reward, terminated, truncated, _ = env.step(action) # gymnasium.step() returns (obs, reward, terminated, truncated, info)\n",
        "            done = terminated or truncated # 'done' is now either terminated or truncated\n",
        "            ep_reward += reward\n",
        "        total_rewards.append(ep_reward)\n",
        "\n",
        "    print(f\"Average Reward (Behavioral Cloning): {np.mean(total_rewards)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_behavioral_cloning()"
      ]
    }
  ]
}