{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN0Um84WccTaxbW4vlqduV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikanta-eng/Reinforcement-learning/blob/main/project_traffic_signal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TRAFFIC SIGNAL OPTIMIZATION USING DEEP Q-LEARNING (DQN)\n",
        "# FULL PROJECT SOURCE CODE IN ONE SINGLE FILE\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================================\n",
        "# 1) TRAFFIC ENVIRONMENT (SIMULATION)\n",
        "# ============================================================\n",
        "\n",
        "class TrafficEnv:\n",
        "    def __init__(self):\n",
        "        self.max_queue = 50\n",
        "        self.arrival_rate = [0.25, 0.15, 0.35, 0.20]     # arrival prob per lane\n",
        "        self.depart_rate_green = 0.70                    # departure prob if green\n",
        "        self.max_time_in_phase = 12\n",
        "        self.n_approaches = 4\n",
        "        self.current_phase = 0                          # 0 = NS green, 1 = EW green\n",
        "        self.time_in_phase = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.queues = [random.randint(0, 3) for _ in range(self.n_approaches)]\n",
        "        self.current_phase = 0\n",
        "        self.time_in_phase = 0\n",
        "        self.t = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        scaled_time = self.time_in_phase / self.max_time_in_phase\n",
        "        return np.array(self.queues + [self.current_phase, scaled_time], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        switched = False\n",
        "\n",
        "        # Action 0 = extend, Action 1 = switch phase\n",
        "        if action == 1:\n",
        "            self.current_phase = 1 - self.current_phase\n",
        "            self.time_in_phase = 0\n",
        "            switched = True\n",
        "        else:\n",
        "            self.time_in_phase += 1\n",
        "            if self.time_in_phase >= self.max_time_in_phase:\n",
        "                self.current_phase = 1 - self.current_phase\n",
        "                self.time_in_phase = 0\n",
        "                switched = True\n",
        "\n",
        "        # Arrivals\n",
        "        for i in range(self.n_approaches):\n",
        "            if random.random() < self.arrival_rate[i]:\n",
        "                self.queues[i] = min(self.queues[i] + 1, self.max_queue)\n",
        "\n",
        "        # Departures\n",
        "        green = [0, 1] if self.current_phase == 0 else [2, 3]\n",
        "        for lane in green:\n",
        "            if self.queues[lane] > 0 and random.random() < self.depart_rate_green:\n",
        "                self.queues[lane] -= 1\n",
        "\n",
        "        # Reward: negative of total queue length\n",
        "        reward = -sum(self.queues)\n",
        "        if switched:\n",
        "            reward -= 1  # small penalty for switching\n",
        "\n",
        "        self.t += 1\n",
        "        done = False\n",
        "        return self._get_state(), reward, done, {\"queues\": self.queues.copy(), \"phase\": self.current_phase}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) DQN NETWORK\n",
        "# ============================================================\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) DQN AGENT\n",
        "# ============================================================\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.gamma = 0.99\n",
        "        self.lr = 1e-3\n",
        "        self.batch_size = 64\n",
        "        self.buffer_size = 20000\n",
        "        self.target_update = 500\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        self.replay = deque(maxlen=self.buffer_size)\n",
        "\n",
        "        self.policy_net = DQNNetwork(state_dim, action_dim).to(device)\n",
        "        self.target_net = DQNNetwork(state_dim, action_dim).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, 1)\n",
        "        state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(state_t)\n",
        "        return int(torch.argmax(q_values).item())\n",
        "\n",
        "    def push(self, s, a, r, s2):\n",
        "        self.replay.append((s, a, r, s2))\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay, self.batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "            expected = rewards + self.gamma * next_q\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, expected)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "def train_model(episodes=100, steps=60):\n",
        "    env = TrafficEnv()\n",
        "    state_dim = 6\n",
        "    action_dim = 2\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "    print(\"\\n=========== TRAINING STARTED ===========\\n\")\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        losses = []\n",
        "\n",
        "        for _ in range(steps):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.push(state, action, reward, next_state)\n",
        "            loss = agent.train_step()\n",
        "            if loss:\n",
        "                losses.append(loss)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        avg_loss = np.mean(losses) if losses else 0.0\n",
        "\n",
        "        print(f\"Episode {ep}/{episodes} | Reward: {total_reward:.2f} \"\n",
        "              f\"| Avg Loss: {avg_loss:.4f} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "        if ep % 50 == 0:\n",
        "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "    torch.save(agent.policy_net.state_dict(), \"traffic_model.pth\")\n",
        "\n",
        "    print(\"\\n=========== TRAINING COMPLETED ===========\")\n",
        "    print(\"Model saved as traffic_model.pth\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(agent, episodes=20, steps=60):\n",
        "    env = TrafficEnv()\n",
        "    total_queues = []\n",
        "\n",
        "    print(\"\\n=========== EVALUATION STARTED ===========\\n\")\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        avg_queue = 0\n",
        "\n",
        "        for _ in range(steps):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            q = agent.policy_net(state_tensor).cpu().detach().numpy()[0]\n",
        "            action = int(np.argmax(q))\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            avg_queue += sum(info[\"queues\"])\n",
        "\n",
        "        avg_queue /= steps\n",
        "        total_queues.append(avg_queue)\n",
        "\n",
        "        print(f\"Evaluation Episode {ep+1}/{episodes} | Avg Queue: {avg_queue:.2f}\")\n",
        "\n",
        "    print(\"\\n=========== EVALUATION COMPLETED ===========\")\n",
        "    print(f\"Final Average Queue Across Episodes: {np.mean(total_queues):.2f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6) MAIN EXECUTION\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = train_model(episodes=100, steps=60)\n",
        "    evaluate_model(agent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v2hjSxqcbqX",
        "outputId": "0af4ab42-9743-4b9a-8180-e552d6a961d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== TRAINING STARTED ===========\n",
            "\n",
            "Episode 1/100 | Reward: -189.00 | Avg Loss: 0.0000 | Epsilon: 1.000\n",
            "Episode 2/100 | Reward: -230.00 | Avg Loss: 6.4716 | Epsilon: 0.751\n",
            "Episode 3/100 | Reward: -299.00 | Avg Loss: 0.9808 | Epsilon: 0.556\n",
            "Episode 4/100 | Reward: -427.00 | Avg Loss: 0.8612 | Epsilon: 0.412\n",
            "Episode 5/100 | Reward: -379.00 | Avg Loss: 0.8552 | Epsilon: 0.305\n",
            "Episode 6/100 | Reward: -306.00 | Avg Loss: 0.8658 | Epsilon: 0.226\n",
            "Episode 7/100 | Reward: -439.00 | Avg Loss: 0.8181 | Epsilon: 0.167\n",
            "Episode 8/100 | Reward: -431.00 | Avg Loss: 0.8487 | Epsilon: 0.124\n",
            "Episode 9/100 | Reward: -541.00 | Avg Loss: 0.8832 | Epsilon: 0.092\n",
            "Episode 10/100 | Reward: -540.00 | Avg Loss: 0.8668 | Epsilon: 0.068\n",
            "Episode 11/100 | Reward: -324.00 | Avg Loss: 0.8325 | Epsilon: 0.050\n",
            "Episode 12/100 | Reward: -585.00 | Avg Loss: 0.8272 | Epsilon: 0.050\n",
            "Episode 13/100 | Reward: -445.00 | Avg Loss: 0.8677 | Epsilon: 0.050\n",
            "Episode 14/100 | Reward: -565.00 | Avg Loss: 0.9078 | Epsilon: 0.050\n",
            "Episode 15/100 | Reward: -376.00 | Avg Loss: 0.8939 | Epsilon: 0.050\n",
            "Episode 16/100 | Reward: -469.00 | Avg Loss: 0.8765 | Epsilon: 0.050\n",
            "Episode 17/100 | Reward: -246.00 | Avg Loss: 0.8902 | Epsilon: 0.050\n",
            "Episode 18/100 | Reward: -507.00 | Avg Loss: 0.8360 | Epsilon: 0.050\n",
            "Episode 19/100 | Reward: -323.00 | Avg Loss: 0.9126 | Epsilon: 0.050\n",
            "Episode 20/100 | Reward: -400.00 | Avg Loss: 0.8817 | Epsilon: 0.050\n",
            "Episode 21/100 | Reward: -272.00 | Avg Loss: 0.8576 | Epsilon: 0.050\n",
            "Episode 22/100 | Reward: -193.00 | Avg Loss: 0.8452 | Epsilon: 0.050\n",
            "Episode 23/100 | Reward: -406.00 | Avg Loss: 0.8939 | Epsilon: 0.050\n",
            "Episode 24/100 | Reward: -512.00 | Avg Loss: 0.9009 | Epsilon: 0.050\n",
            "Episode 25/100 | Reward: -206.00 | Avg Loss: 0.8571 | Epsilon: 0.050\n",
            "Episode 26/100 | Reward: -498.00 | Avg Loss: 0.8216 | Epsilon: 0.050\n",
            "Episode 27/100 | Reward: -534.00 | Avg Loss: 0.8957 | Epsilon: 0.050\n",
            "Episode 28/100 | Reward: -303.00 | Avg Loss: 0.8497 | Epsilon: 0.050\n",
            "Episode 29/100 | Reward: -178.00 | Avg Loss: 0.8656 | Epsilon: 0.050\n",
            "Episode 30/100 | Reward: -496.00 | Avg Loss: 0.8740 | Epsilon: 0.050\n",
            "Episode 31/100 | Reward: -349.00 | Avg Loss: 0.8474 | Epsilon: 0.050\n",
            "Episode 32/100 | Reward: -254.00 | Avg Loss: 0.8733 | Epsilon: 0.050\n",
            "Episode 33/100 | Reward: -505.00 | Avg Loss: 0.8778 | Epsilon: 0.050\n",
            "Episode 34/100 | Reward: -283.00 | Avg Loss: 0.8777 | Epsilon: 0.050\n",
            "Episode 35/100 | Reward: -568.00 | Avg Loss: 0.8799 | Epsilon: 0.050\n",
            "Episode 36/100 | Reward: -709.00 | Avg Loss: 0.8675 | Epsilon: 0.050\n",
            "Episode 37/100 | Reward: -575.00 | Avg Loss: 0.9070 | Epsilon: 0.050\n",
            "Episode 38/100 | Reward: -476.00 | Avg Loss: 0.9024 | Epsilon: 0.050\n",
            "Episode 39/100 | Reward: -635.00 | Avg Loss: 0.8715 | Epsilon: 0.050\n",
            "Episode 40/100 | Reward: -362.00 | Avg Loss: 0.9177 | Epsilon: 0.050\n",
            "Episode 41/100 | Reward: -497.00 | Avg Loss: 0.8843 | Epsilon: 0.050\n",
            "Episode 42/100 | Reward: -402.00 | Avg Loss: 0.8549 | Epsilon: 0.050\n",
            "Episode 43/100 | Reward: -264.00 | Avg Loss: 0.9189 | Epsilon: 0.050\n",
            "Episode 44/100 | Reward: -611.00 | Avg Loss: 0.9162 | Epsilon: 0.050\n",
            "Episode 45/100 | Reward: -304.00 | Avg Loss: 0.9240 | Epsilon: 0.050\n",
            "Episode 46/100 | Reward: -730.00 | Avg Loss: 0.9059 | Epsilon: 0.050\n",
            "Episode 47/100 | Reward: -538.00 | Avg Loss: 0.8973 | Epsilon: 0.050\n",
            "Episode 48/100 | Reward: -322.00 | Avg Loss: 0.8652 | Epsilon: 0.050\n",
            "Episode 49/100 | Reward: -330.00 | Avg Loss: 0.9075 | Epsilon: 0.050\n",
            "Episode 50/100 | Reward: -320.00 | Avg Loss: 0.9071 | Epsilon: 0.050\n",
            "Episode 51/100 | Reward: -254.00 | Avg Loss: 7.7357 | Epsilon: 0.050\n",
            "Episode 52/100 | Reward: -363.00 | Avg Loss: 3.4662 | Epsilon: 0.050\n",
            "Episode 53/100 | Reward: -198.00 | Avg Loss: 3.2871 | Epsilon: 0.050\n",
            "Episode 54/100 | Reward: -176.00 | Avg Loss: 3.4283 | Epsilon: 0.050\n",
            "Episode 55/100 | Reward: -126.00 | Avg Loss: 3.3641 | Epsilon: 0.050\n",
            "Episode 56/100 | Reward: -225.00 | Avg Loss: 3.3662 | Epsilon: 0.050\n",
            "Episode 57/100 | Reward: -333.00 | Avg Loss: 3.3554 | Epsilon: 0.050\n",
            "Episode 58/100 | Reward: -226.00 | Avg Loss: 3.2700 | Epsilon: 0.050\n",
            "Episode 59/100 | Reward: -271.00 | Avg Loss: 3.1400 | Epsilon: 0.050\n",
            "Episode 60/100 | Reward: -250.00 | Avg Loss: 3.2529 | Epsilon: 0.050\n",
            "Episode 61/100 | Reward: -344.00 | Avg Loss: 3.2499 | Epsilon: 0.050\n",
            "Episode 62/100 | Reward: -232.00 | Avg Loss: 3.2436 | Epsilon: 0.050\n",
            "Episode 63/100 | Reward: -253.00 | Avg Loss: 3.2512 | Epsilon: 0.050\n",
            "Episode 64/100 | Reward: -137.00 | Avg Loss: 3.3148 | Epsilon: 0.050\n",
            "Episode 65/100 | Reward: -156.00 | Avg Loss: 3.1404 | Epsilon: 0.050\n",
            "Episode 66/100 | Reward: -376.00 | Avg Loss: 3.1293 | Epsilon: 0.050\n",
            "Episode 67/100 | Reward: -391.00 | Avg Loss: 3.2278 | Epsilon: 0.050\n",
            "Episode 68/100 | Reward: -232.00 | Avg Loss: 3.1417 | Epsilon: 0.050\n",
            "Episode 69/100 | Reward: -510.00 | Avg Loss: 3.1255 | Epsilon: 0.050\n",
            "Episode 70/100 | Reward: -236.00 | Avg Loss: 3.1884 | Epsilon: 0.050\n",
            "Episode 71/100 | Reward: -368.00 | Avg Loss: 3.2807 | Epsilon: 0.050\n",
            "Episode 72/100 | Reward: -175.00 | Avg Loss: 3.2921 | Epsilon: 0.050\n",
            "Episode 73/100 | Reward: -273.00 | Avg Loss: 3.1835 | Epsilon: 0.050\n",
            "Episode 74/100 | Reward: -486.00 | Avg Loss: 3.2181 | Epsilon: 0.050\n",
            "Episode 75/100 | Reward: -225.00 | Avg Loss: 3.2856 | Epsilon: 0.050\n",
            "Episode 76/100 | Reward: -277.00 | Avg Loss: 3.1768 | Epsilon: 0.050\n",
            "Episode 77/100 | Reward: -254.00 | Avg Loss: 3.3347 | Epsilon: 0.050\n",
            "Episode 78/100 | Reward: -188.00 | Avg Loss: 3.2883 | Epsilon: 0.050\n",
            "Episode 79/100 | Reward: -355.00 | Avg Loss: 3.2769 | Epsilon: 0.050\n",
            "Episode 80/100 | Reward: -309.00 | Avg Loss: 3.3360 | Epsilon: 0.050\n",
            "Episode 81/100 | Reward: -402.00 | Avg Loss: 3.2748 | Epsilon: 0.050\n",
            "Episode 82/100 | Reward: -566.00 | Avg Loss: 3.1408 | Epsilon: 0.050\n",
            "Episode 83/100 | Reward: -263.00 | Avg Loss: 3.2076 | Epsilon: 0.050\n",
            "Episode 84/100 | Reward: -228.00 | Avg Loss: 3.2297 | Epsilon: 0.050\n",
            "Episode 85/100 | Reward: -144.00 | Avg Loss: 3.2757 | Epsilon: 0.050\n",
            "Episode 86/100 | Reward: -149.00 | Avg Loss: 3.2714 | Epsilon: 0.050\n",
            "Episode 87/100 | Reward: -367.00 | Avg Loss: 3.1633 | Epsilon: 0.050\n",
            "Episode 88/100 | Reward: -170.00 | Avg Loss: 3.3038 | Epsilon: 0.050\n",
            "Episode 89/100 | Reward: -346.00 | Avg Loss: 3.1369 | Epsilon: 0.050\n",
            "Episode 90/100 | Reward: -138.00 | Avg Loss: 3.0211 | Epsilon: 0.050\n",
            "Episode 91/100 | Reward: -350.00 | Avg Loss: 3.2058 | Epsilon: 0.050\n",
            "Episode 92/100 | Reward: -170.00 | Avg Loss: 3.1707 | Epsilon: 0.050\n",
            "Episode 93/100 | Reward: -182.00 | Avg Loss: 3.2096 | Epsilon: 0.050\n",
            "Episode 94/100 | Reward: -118.00 | Avg Loss: 3.2364 | Epsilon: 0.050\n",
            "Episode 95/100 | Reward: -385.00 | Avg Loss: 3.2720 | Epsilon: 0.050\n",
            "Episode 96/100 | Reward: -197.00 | Avg Loss: 3.1727 | Epsilon: 0.050\n",
            "Episode 97/100 | Reward: -347.00 | Avg Loss: 3.0694 | Epsilon: 0.050\n",
            "Episode 98/100 | Reward: -328.00 | Avg Loss: 3.2676 | Epsilon: 0.050\n",
            "Episode 99/100 | Reward: -149.00 | Avg Loss: 3.0727 | Epsilon: 0.050\n",
            "Episode 100/100 | Reward: -331.00 | Avg Loss: 3.2015 | Epsilon: 0.050\n",
            "\n",
            "=========== TRAINING COMPLETED ===========\n",
            "Model saved as traffic_model.pth\n",
            "\n",
            "=========== EVALUATION STARTED ===========\n",
            "\n",
            "Evaluation Episode 1/20 | Avg Queue: 2.50\n",
            "Evaluation Episode 2/20 | Avg Queue: 2.97\n",
            "Evaluation Episode 3/20 | Avg Queue: 4.82\n",
            "Evaluation Episode 4/20 | Avg Queue: 11.88\n",
            "Evaluation Episode 5/20 | Avg Queue: 3.97\n",
            "Evaluation Episode 6/20 | Avg Queue: 5.05\n",
            "Evaluation Episode 7/20 | Avg Queue: 3.68\n",
            "Evaluation Episode 8/20 | Avg Queue: 8.32\n",
            "Evaluation Episode 9/20 | Avg Queue: 3.17\n",
            "Evaluation Episode 10/20 | Avg Queue: 3.20\n",
            "Evaluation Episode 11/20 | Avg Queue: 1.68\n",
            "Evaluation Episode 12/20 | Avg Queue: 2.77\n",
            "Evaluation Episode 13/20 | Avg Queue: 4.88\n",
            "Evaluation Episode 14/20 | Avg Queue: 2.80\n",
            "Evaluation Episode 15/20 | Avg Queue: 3.00\n",
            "Evaluation Episode 16/20 | Avg Queue: 3.62\n",
            "Evaluation Episode 17/20 | Avg Queue: 4.33\n",
            "Evaluation Episode 18/20 | Avg Queue: 6.47\n",
            "Evaluation Episode 19/20 | Avg Queue: 2.95\n",
            "Evaluation Episode 20/20 | Avg Queue: 3.20\n",
            "\n",
            "=========== EVALUATION COMPLETED ===========\n",
            "Final Average Queue Across Episodes: 4.26\n"
          ]
        }
      ]
    }
  ]
}